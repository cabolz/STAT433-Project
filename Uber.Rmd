---
title: "STAT433 Final Project Report: Uber Pickups in NYC"
author: "Caitlin Bolz, Matt Voss, Steven Xia"
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(readr)
library(tidyverse)
library(leaps)
library(DAAG)
library(recipes)
library(stringr)
library(tidytext)
library(ggplot2)
library(ggrepel)
library(lubridate)
library(sp)
library(tigris)
library(dplyr)
library(leaflet)
library(sp)
library(ggmap)
library(maptools)
library(broom)
library(httr)
library(rgdal)
```

The group’s GitHub repository is located at this [link](https://github.com/cabolz/STAT433-Project.git). The group’s shiny application is hosted on a shinyapps.io website and can be found at this [link](https://cabolz.shinyapps.io/STAT433-Project/).

**Introduction**: In recent years, the emergence of the ridesharing application, Uber, has drastically altered the transportation industry. Just a decade ago, the streets of Manhattan were filled with yellow taxi cabs. Today, there are nearly four times as many ridesharing vehicles on the road than taxis. As Uber has led the taxi industry into a rapid decline, the group is interested in exploring its usage patterns across New York City. Specifically, the group is interested in determining the specific factors (such as time, weather, location, etc.) that have the most influence on the number of Uber pickups in NYC. This knowledge is important because it will not only help future ridesharing customers better coordinate their trip experiences, but will also assist Uber and other ridesharing companies in developing more efficient, accurate algorithms and software tailored to optimize the amount of trips purchased from their application.
Based on the data analysis and visualizations, the group reasonably concludes that the two factors that have the greatest impact on Uber usage levels in NYC during May 2014 are Borough Code and Hour .

**Data**:  The group collected over 624,000 rows of data from two main sources. The first subset of data originates from the NYC Taxi and Limousine Commission (TLC), and consists of 4.5 million pickups from April to September 2014 as well as 14.3 million pickups from January to June 2015. However, due to limited computer processing power, the group decided to only use Uber pickup data from May 2014. For each unique Uber trip in the first data source, there exists several spatial and temporal variables, such as ‘Hour’, ‘Day’, ‘Longitude’, ‘Latitude’, and ‘Burough’. The group’s first data source can be found at this [link](https://data.world/data-society/uber-pickups-in-nyc).

The group’s second data source consists of weather information in NYC from 2012 to 2017. This weather data was originally collected by David Beniaguev of Hebrew University in Telaviv, Israel. Each entry in this dataset represents a single hour from a given day. Because we are only interested in the Uber trips that occurred in May 2014, we also took a subset of this second data source, only looking at weather information within May 2014. For each hour within this dataset, there is an extensive amount of weather variables such as ‘Temperature’, ‘Sky Description’, ‘Pressure’, ‘Wind Speed’, and ‘Humidity’. The group’s second data source can be found at this [link](https://www.kaggle.com/selfishgene/historical-hourly-weather-data?select=humidity.csv).

After the group obtained a large 2012-2017 weather dataset and cut out only the entries from 2014, this one year of NYC weather data was joined with the initial Uber pickup dataset. After inspecting our joined dataset we then found several empty entries within the ‘Humidity’ variable, originally from the weather dataset. Thus, we removed all NA values. Lastly, the group’s joined dataset was filtered for only entries during the month of May as the group had specified this timeframe.

**Methods**: The group was initially curious about the amount of Uber usage during weekdays as compared to weekends. The initial analysis of our data included a bar graph that visualized the number of Uber pickups for each day within NYC during the month of May 2014. Next, to determine the strength of each predictor, a linear model was fitted. This was followed by checking for multicollinearity to confirm that each predictor was truly independent. The group then proceeded to calculate Root Mean Squared Error for Prediction (RMSEP), Mean Squared Error of Prediction (MSEP), and R^2 correlation coefficient. Lastly, the top 5 principal components of the dataset were calculated and visualized, and a final model was created for predicting the number of pickups using our spatial, temporal, and weather variables.

```{r message=FALSE, include=FALSE}
may<-read_csv("uber-raw-data-may14.csv")
cleanmay = may %>% 
  separate("Date/Time", into =c("Date","Time"), sep= " ") %>% 
  mutate(dayOfWeek = Date) %>% 
  mutate(dayOfWeek = mdy(Date)) %>% 
  mutate(dayOfWeek = wday(dayOfWeek)) %>% 
  separate(Date, into = c("month","day","year"),sep="/") %>% 
  separate(Time, into = c("hour", "minute"),sep=":")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
hum<-read_csv("humidity.csv")
Humidityin2014<-hum %>% 
  separate("datetime", into = c("Date","hour"), sep = " ") %>% 
  separate("Date", into = c("month","day","year"),sep="/") %>% 
  filter(year==14) %>% 
  mutate(Humidity = `New York`) %>% 
  transmute(hour,day,month,year,Humidity)
Sky<-read_csv("weather_description.csv")
JustNewYorkSky<-Sky %>% 
  separate("datetime", into = c("Date","hour"), sep = " ") %>% 
  separate("Date", into = c("month","day","year"), sep="/") %>% 
  filter(year==14) %>% 
  mutate(SkyDescription=`New York`) %>% 
  transmute(hour,day,month,year,SkyDescription)
temp<-read_csv("temperature.csv")
JustTemp<-temp %>% 
  separate("datetime", into = c("Date","hour"), sep=" ") %>% 
  separate("Date", into = c("month","day","year"),sep="/") %>% 
  filter(year==14) %>% 
  transmute(hour,day,month,year,`New York`) %>% 
  mutate(temperature=((`New York`-273.15)*9/5)+32) %>% 
  transmute(hour,day,month,year,temperature)
pre<- read_csv("pressure.csv")
pressur<- pre %>% 
  separate("datetime", into = c("Date","hour"), sep = " ") %>% 
  separate("Date", into = c("month","day","year"), sep="/") %>% 
  filter(year==14) %>% 
  mutate(pressure=`New York`) %>% 
  transmute(hour,day,month,year,pressure)
win<- read_csv("wind_speed.csv")
win<- win %>% 
  separate("datetime", into = c("Date","hour"), sep = " ") %>% 
  separate("Date", into = c("month","day","year"), sep="/") %>% 
  filter(year==14) %>% 
  mutate(windspeed=`New York`) %>% 
  transmute(hour,day,month,year,windspeed)
d<-full_join(Humidityin2014,JustNewYorkSky)
temporary<- full_join(win,pressur) %>% 
  full_join(d)
weatherData<-full_join(temporary,JustTemp) %>% 
  separate(hour,into = c("hour"),sep = ":")
weatherData<- weatherData %>%  
  transmute(hour,day,month,windspeed,pressure,Humidity,SkyDescription,temperature)

weather =weatherData %>% 
  filter(month==5) %>% 
  transmute(hour,day,temperature,pressure,windspeed,Humidity) %>% 
  mutate(hour = as.double(hour)) %>% 
  mutate(day = as.double(day))
```


```{r warning=FALSE, include=FALSE}
UberandWeather <- cleanmay %>% 
  full_join(weatherData) 
UberandWeather <- subset(UberandWeather, select = c("hour", "minute", "Lat", "Lon", "month", "day", "Humidity", "SkyDescription", "temperature", "pressure","windspeed","dayOfWeek"))
UberandWeather <- na.omit(UberandWeather)
```

```{r include=FALSE}
r <- GET('http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson')
nyc_neighborhoods <- readOGR(content(r,'text'), 'OGRGeoJSON', verbose = F)
uw = UberandWeather
lats <- uw$Lat
lngs <- uw$Lon
points <- data.frame(lat=lats, lng=lngs)
points_spdf <- points
coordinates(points_spdf) <- ~lng + lat
proj4string(points_spdf) <- proj4string(nyc_neighborhoods)
matches <- over(points_spdf, nyc_neighborhoods)
points <- cbind(points, matches)
points$Lat = points$lat
points$Lon = points$lng
fullData = points %>% 
  left_join(uw)
fullData = distinct(fullData)
fullData = fullData %>% 
  transmute(Lat,Lon,neighborhood,boroughCode,borough,month,day,hour,minute,dayOfWeek,
            windspeed,pressure,Humidity,SkyDescription,temperature)
```
# Results

#### figure 1
```{r echo=FALSE}
sum<-UberandWeather %>%
  filter(month == 5) %>% 
  group_by(day) %>% 
  summarize(pickup=n(), temp = mean(temperature))
sum$day = as.numeric(sum$day)
ggplot(sum, aes(x = day, y = pickup, fill = temp)) +
  geom_col() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(
    title = "Number of Uber Pickups in NYC in May 2014",
    x = "Day of the Month",
    y = "Number of Pickups",
    fill = "Temperature") +
  scale_x_continuous(breaks = waiver(), n.breaks = 6)
```



```{r include=FALSE}
fullData = fullData %>%
  mutate( 
    borough = case_when(
      str_detect(borough, "Brooklyn") ~ paste("Kings"),
      str_detect(borough, "Manhattan") ~ paste("New York"),
      str_detect(borough, "Staten Island") ~ paste("Richmond"),
      str_detect(borough, "Queens") ~ paste("Queens"),
      str_detect(borough, "Bronx") ~ paste("Bronx")))

tab = fullData %>% 
  transmute(borough, neighborhood, day,neighborhood) %>% 
  na.omit(borough) %>% 
  group_by(day,borough) %>% 
  summarize(num_pickups = n()) %>% 
  pivot_wider(names_from = borough, values_from = num_pickups)
tab
```


```{r include=FALSE}
Yhat = fullData %>% 
  na.omit(boroughCode) %>% 
  group_by(boroughCode,hour,day,dayOfWeek) %>% 
  summarize(pickup = n()) %>% 
  mutate(hour = as.double(hour)) %>% 
  mutate(day = as.double(day)) 

# do the join separate
Ytilda = Yhat %>% full_join(weather) %>% mutate(boroughCode=as.double(boroughCode)) %>% 
  mutate(pickup = as.double(pickup)) %>% na.omit(Yhat) %>% 
  ungroup
Ytilda
```



```{r include=FALSE}
Yapp = fullData %>% 
  na.omit(boroughCode) %>% 
  group_by(borough,hour,day,dayOfWeek) %>% 
  summarize(pickup = n()) %>% 
  mutate(hour = as.double(hour)) %>% 
  mutate(day = as.double(day)) 

Yapp1 = Yapp %>% 
  filter(day == 1 | day == 2 | day == 3) %>% 
  group_by(borough) %>% 
  summarize(pickups = sum(pickup))

Yapp2 = Yapp %>% 
  filter(day == 4 | day == 5 | day == 6 | day == 7 | day == 8 | day == 9 | day == 10) %>% 
  group_by(borough) %>% 
  summarize(pickups = sum(pickup))

Yapp3 = Yapp %>% 
  filter(day == 11 | day == 12 | day == 13 | day == 14 | day == 15 | day == 16 | day == 17) %>% 
  group_by(borough) %>% 
  summarize(pickups = sum(pickup))

Yapp4 = Yapp %>% 
  filter(day == 18 | day == 19 | day == 20 | day == 21 | day == 22 | day == 23 | day == 24) %>% 
  group_by(borough) %>% 
  summarize(pickups = sum(pickup))

Yapp5 = Yapp %>% 
  filter(day == 25 | day == 26 | day == 27 | day == 28 | day == 29 | day == 30 | day == 31) %>% 
  group_by(borough) %>% 
  summarize(pickups = sum(pickup))
```


```{r include=FALSE}
borough = c("Manhattan","Brooklyn","Queens","Bronx","Staten Island")

# Get lat/long coordinates for NY counties
choropleth = map_data("county") %>% 
  filter(region == "new york") %>% 
  select(-group, -order, - region) %>% 
  rename(County = subregion) %>% 
  mutate(County = str_to_title(County)) %>% 
  filter(County == "New York" | County == "Kings" | County == "Queens" | 
           County == "Bronx" | County == "Richmond") %>% 
  rename(borough = County)
```


```{r include=FALSE}
choropleth1 = choropleth %>% 
  left_join(Yapp1, by = c("borough"))

choropleth2 = choropleth %>% 
  left_join(Yapp2, by = c("borough"))

choropleth3 = choropleth %>% 
  left_join(Yapp3, by = c("borough"))

choropleth4 = choropleth %>% 
  left_join(Yapp4, by = c("borough"))

choropleth5 = choropleth %>% 
  left_join(Yapp5, by = c("borough"))
```

```{r include=FALSE}
write_csv(choropleth1, "choropleth1.csv")
write_csv(choropleth2, "choropleth2.csv")
write_csv(choropleth3, "choropleth3.csv")
write_csv(choropleth4, "choropleth4.csv")
write_csv(choropleth5, "choropleth5.csv")
```


#### figure 2
```{r echo=FALSE}
library(ggplot2)
ggplot(data = Ytilda, mapping = aes(x =hour, y = pickup))+geom_point(mapping = aes(color = day))+facet_wrap(~boroughCode)
```

#### Linear Model
```{r echo=FALSE}
lin<-lm(pickup~.,data = Ytilda)
summary(lin)
```

#### PCA
```{r include=FALSE}
pca_rep = recipe(pickup~., data = Ytilda) %>% 
  update_role(day, hour, new_role = "id") %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors())

pca_prep = prep(pca_rep)

components = tidy(pca_prep, 2)
scores = juice(pca_prep)
variances = tidy(pca_prep, 2, type = "variance")

components = components %>%
  filter(component %in% str_c("PC", 1:6)) %>% 
  mutate(terms = reorder_within(terms, abs(value), component))
```

#### plot of variance
```{r echo=FALSE}
variances = variances %>% filter(terms == "percent variance")
ggplot(variances) +  geom_col(aes(component, value))
```

#### plot of top 6 components
```{r echo=FALSE}
ggplot(components, aes(value, terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ component, scales = "free_y") +
  scale_y_reordered() +
  labs(y = NULL) +
  theme(axis.text = element_text(size = 10), strip.text = element_text(size = 10))
```

After mapping the plot of variance, the group determined that six Principal Components is the most prudent number of components to both reduce dimensionality and variance for the model. Next, the group constructed component graphs to understand how the original factors impacted each component. We interpret the components of our Principal Components by looking at the linear coefficients of the variables used to define them. For example, we can see that the first Principal Component positively captures humidity and day of the week and negatively captures the temperature and wind speed. From here, one can see which factor impacts the value of the principal component the most. Those near zero had little effect on the individual principal component and those near the extremes had negative or positive effects on the component based on its dependent axis location. As observed from the Principal Components Analysis, one can see that PC1 is mostly about temperature and humidity, PC2 is mostly pressure, PC3 is mostly boroughCode, PC4 is mostly wind speed and day of the week. PC5 is mostly temperature, and similar to PC1, PC6 is related to both humidity and temperature.   


#### Scores
```{r echo=FALSE}
ggplot(scores, aes(PC1, PC2, label = hour)) +
  geom_point(aes(color = hour), alpha = 1, size = 1) +
  scale_color_viridis_c()

ggplot(scores, aes(PC3, PC4, label = hour)) +
  geom_point(aes(color = hour), alpha = 1, size = 1) +
  scale_color_viridis_c()

ggplot(scores, aes(PC2, PC4, label = hour)) +
  geom_point(aes(color = hour), alpha = 1, size = 1) +
  scale_color_viridis_c()

ggplot(scores, aes(PC3, PC5, label = hour)) +
  geom_point(aes(color = hour), alpha = 1, size = 1) +
  scale_color_viridis_c()
```

# Conclusion 

The group determined that Hour and Borough Code are the most influential factors in determining the number of Uber pickups for May 2014 in NYC. The group notices the high p-values in the linear regression for Hour and Borough Code and have also noticed the third principal component was mostly influenced by Borough Code. From Figure 1, the group also notices that there exist clear patterns of fluctuation in the number of Uber pickups between each day of the week. This pattern is consistent across all four weeks in May 2014. In Figure 2, there is a considerable difference in the graphs based on the borough and can see in Borough Code 1 the massive increase in pickups later in the day or early in the morning. 



```{r eval=FALSE, include=FALSE}
library(olsrr)
model1 = lm(pickup~., data = Ytilda)
summary(model1)

ols_vif_tol(model1)
ols_eigen_cindex(model1)
ols_coll_diag(model1)

library(pls)
pcr_model = pcr(pickup~., data = Ytilda, scale = T, center = T, validation = "CV")
summary(pcr_model)

par(mfrow=c(1,3))
validationplot(pcr_model)
validationplot(pcr_model, val.type="MSEP")
validationplot(pcr_model, val.type = "R2")

uber_model = pcr(pickup~., data = Ytilda, scale = T, center = T, ncomp = 5)
 names(uber_model)
Beta = coef(uber_model,ncomp = 5, intercept = T)
Beta
```


```{r eval=FALSE, include=FALSE}
library(plotly)
library(rjson)
r1 = 'http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson'
geojson <- rjson::fromJSON(file=r1)
full = fullData %>% 
  transmute(borough,neighborhood,day) %>% 
  group_by(borough,day,neighborhood) %>% 
  summarize(num_points= n(),borough,day)
full = distinct(full)
full
fig <- plot_ly() 
fig <- fig %>% add_trace(
  type="choroplethmapbox",
  geojson=geojson,
  locations=full$neighborhood,
  z=full$num_points,
  colorscale="Viridis",
  featureidkey = "properties.neighborhood"
)
fig <- fig %>% layout(
  mapbox=list(
    style="carto-positron",
    zoom =9,
    center=list(lon = -73.98,lat =  40.75))
)
fig

```
